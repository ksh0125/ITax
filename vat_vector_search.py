import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any
import traceback

class VATVectorSearch:
    def __init__(self, model_name: str = "jhgan/ko-sbert-nli", data_file: str = "vat_law_processed.pkl"):
        """ë¶€ê°€ê°€ì¹˜ì„¸ë²• ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""
        print("ğŸš€ ë¶€ê°€ê°€ì¹˜ì„¸ë²• ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # ëª¨ë¸ ë¡œë”©
        print("â³ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            self.model = SentenceTransformer(model_name)
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        except Exception as model_error:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_error}")
            raise
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©
        self.data = self._load_data(data_file)
        self.embeddings_matrix = self._create_embeddings_matrix()
        
        print(f"âœ… ê²€ìƒ‰ ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ: {len(self.data)}ê°œ ì²­í¬")
    
    def _load_data(self, data_file: str) -> List[Dict]:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ '{data_file}' ë¡œë”© ì¤‘...")
        try:
            with open(data_file, 'rb') as f:
                data = pickle.load(f)
            print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(data)}ê°œ ì²­í¬")
            return data
        except FileNotFoundError:
            print(f"âŒ '{data_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("   ë¨¼ì € 'python vat_preprocessor.py'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return []
        except Exception as load_error:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {load_error}")
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return []
    
    def _create_embeddings_matrix(self) -> np.ndarray:
        """ì„ë² ë”©ì„ numpy í–‰ë ¬ë¡œ ë³€í™˜"""
        if not self.data:
            return np.array([])
        
        print("ğŸ”¢ ì„ë² ë”© í–‰ë ¬ ìƒì„± ì¤‘...")
        try:
            embeddings = [chunk['embedding'] for chunk in self.data]
            matrix = np.array(embeddings)
            print(f"âœ… ì„ë² ë”© í–‰ë ¬ ìƒì„± ì™„ë£Œ: {matrix.shape}")
            return matrix
        except Exception as matrix_error:
            print(f"âŒ ì„ë² ë”© í–‰ë ¬ ìƒì„± ì˜¤ë¥˜: {matrix_error}")
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return np.array([])
    
    def search(self, query: str, top_k: int = 10, similarity_threshold: float = 0.1) -> List[Dict]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰"""
        if not self.data or self.embeddings_matrix.size == 0:
            print("âŒ ê²€ìƒ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        print(f"ğŸ” '{query}' ê²€ìƒ‰ ì¤‘...")
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.model.encode([query])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_embedding, self.embeddings_matrix)[0]
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similar_indices = np.argsort(similarities)[::-1]
            
            results = []
            for idx in similar_indices[:top_k]:
                similarity = similarities[idx]
                
                # ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í¬í•¨
                if similarity >= similarity_threshold:
                    chunk_data = self.data[idx].copy()
                    chunk_data['similarity'] = float(similarity)
                    results.append(chunk_data)
            
            print(f"âœ… {len(results)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
            return results
            
        except Exception as search_error:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {search_error}")
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return []
    
    def search_and_aggregate(self, query: str, top_k: int = 10) -> Dict[str, Any]:
        """ê²€ìƒ‰ í›„ ì¡°ë¬¸ë³„ë¡œ ì§‘ê³„"""
        try:
            chunks = self.search(query, top_k * 2)  # ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ì§‘ê³„
            
            if not chunks:
                return {
                    'query': query,
                    'total_chunks_found': 0,
                    'unique_articles': 0,
                    'results': []
                }
            
            # ì¡°ë¬¸ë³„ë¡œ ê·¸ë£¹í™”
            article_groups = {}
            for chunk in chunks:
                article_key = f"{chunk['law_name']}_{chunk['article_number']}"
                
                if article_key not in article_groups:
                    article_groups[article_key] = {
                        'law_name': chunk['law_name'],
                        'article_number': chunk['article_number'],
                        'article_title': chunk['article_title'],
                        'full_content': chunk['full_content'],
                        'max_similarity': chunk['similarity'],
                        'avg_similarity': chunk['similarity'],
                        'chunk_count': 1,
                        'relevant_chunks': [chunk['chunk_content']]
                    }
                else:
                    group = article_groups[article_key]
                    group['max_similarity'] = max(group['max_similarity'], chunk['similarity'])
                    group['avg_similarity'] = (group['avg_similarity'] * group['chunk_count'] + chunk['similarity']) / (group['chunk_count'] + 1)
                    group['chunk_count'] += 1
                    group['relevant_chunks'].append(chunk['chunk_content'])
            
            # ìµœê³  ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            aggregated_results = list(article_groups.values())
            aggregated_results.sort(key=lambda x: x['max_similarity'], reverse=True)
            
            return {
                'query': query,
                'total_chunks_found': len(chunks),
                'unique_articles': len(aggregated_results),
                'results': aggregated_results[:top_k]
            }
            
        except Exception as aggregate_error:
            print(f"âŒ ì§‘ê³„ ì˜¤ë¥˜: {aggregate_error}")
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return {
                'query': query,
                'total_chunks_found': 0,
                'unique_articles': 0,
                'results': [],
                'error': str(aggregate_error)
            }
    
    def get_statistics(self):
        """ê²€ìƒ‰ ì—”ì§„ í†µê³„"""
        try:
            if not self.data:
                return {"error": "ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
            
            article_count = len(set(chunk['article_number'] for chunk in self.data))
            
            return {
                "ì´_ì²­í¬ìˆ˜": len(self.data),
                "ì´_ì¡°ë¬¸ìˆ˜": article_count,
                "ì„ë² ë”©_ì°¨ì›": self.data[0]['embedding_dim'] if self.data else 0,
                "ëª¨ë¸ëª…": "jhgan/ko-sbert-nli",
                "ìƒíƒœ": "ì¤€ë¹„ì™„ë£Œ"
            }
        except Exception as stats_error:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {stats_error}")
            return {"error": f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(stats_error)}"}

def test_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    try:
        search_engine = VATVectorSearch()
        
        if not search_engine.data:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ë¶€ê°€ê°€ì¹˜ì„¸ ì„¸ìœ¨",
            "ì‚¬ì—…ì ì •ì˜",
            "ì¬í™” ê³µê¸‰",
            "ë‚©ì„¸ì˜ë¬´ì"
        ]
        
        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
            print('='*60)
            
            results = search_engine.search_and_aggregate(query, top_k=3)
            
            if 'error' in results:
                print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {results['error']}")
                continue
            
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {results['total_chunks_found']}ê°œ ì²­í¬, {results['unique_articles']}ê°œ ì¡°ë¬¸")
            
            for i, result in enumerate(results['results'], 1):
                print(f"\n{i}. [{result['law_name']}] {result['article_number']}")
                print(f"   ğŸ“‹ ì œëª©: {result['article_title']}")
                print(f"   ğŸ¯ ìµœëŒ€ ìœ ì‚¬ë„: {result['max_similarity']:.4f}")
                print(f"   ğŸ“Š í‰ê·  ìœ ì‚¬ë„: {result['avg_similarity']:.4f}")
                print(f"   ğŸ”¢ ë§¤ì¹­ ì²­í¬ ìˆ˜: {result['chunk_count']}")
                print(f"   ğŸ“„ ë‚´ìš©: {result['full_content'][:200]}...")
                
    except Exception as test_error:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {test_error}")
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ§ª ë¶€ê°€ê°€ì¹˜ì„¸ë²• ë²¡í„° ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        search_engine = VATVectorSearch()
        
        # í†µê³„ ì¶œë ¥
        stats = search_engine.get_statistics()
        if "error" not in stats:
            print(f"ğŸ“Š ê²€ìƒ‰ ì—”ì§„ í†µê³„:")
            for key, value in stats.items():
                print(f"   {key}: {value}")
        else:
            print(f"âŒ í†µê³„ ì˜¤ë¥˜: {stats['error']}")
            return
        
        # ëŒ€í™”í˜• ê²€ìƒ‰
        print("\nğŸ’¬ ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œì‘ (ì¢…ë£Œ: 'quit')")
        while True:
            try:
                query = input("\nğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("ğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not query:
                    continue
                
                results = search_engine.search_and_aggregate(query, top_k=3)
                
                if 'error' in results:
                    print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {results['error']}")
                    continue
                
                if not results['results']:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                print(f"\nğŸ“‹ '{query}' ê²€ìƒ‰ ê²°ê³¼:")
                print("-" * 50)
                
                for i, result in enumerate(results['results'], 1):
                    print(f"\n{i}. {result['article_number']} {result['article_title']}")
                    print(f"   ìœ ì‚¬ë„: {result['max_similarity']:.3f}")
                    print(f"   ë‚´ìš©: {result['full_content'][:150]}...")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as input_error:
                print(f"âŒ ì…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜: {input_error}")
                
    except Exception as main_error:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {main_error}")
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")

if __name__ == "__main__":
    main()